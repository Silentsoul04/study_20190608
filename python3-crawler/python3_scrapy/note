# 几乎每个网站都有robots.txt，里面设定了哪些路径允许爬取
# https://www.taobao.com/robots.txt

# User-agent:  Baiduspider
Allow:  /article
Allow:  /oshtml
Allow:  /ershou
Allow: /$
Disallow:  /product/
Disallow:  /

https://www.jianshu.com/p/6bc5a4641629
https://www.jianshu.com/p/107445b4fffd
https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/logging.html

http://www.scrapyd.cn/

Install packages one by one: lxml,pyOpenSSL,Twisted,PyWin32,scrapy

爬虫框架scrapy
How to use scrapy in pycharm
https://blog.csdn.net/YiXiao1997/article/details/87834960

# https://blog.csdn.net/cdyjy_litao/article/details/78607124
## windows平台下关于环境变量的知识

1. cd D:\xiaozhan_git\study_20190608\python3-crawler\python3_scrapy
website: http://quotes.toscrape.com/
基本操作步骤如下:
scrapy startproject quotetutorial       //新建一个项目
cd quotetutorial                        //进入项目所在目录
scrapy genspider quotes quotes.toscrape.com          //生成一个和上面同名的目录,创建了一个spider，名字为quotes
scrapy genspider baidu www.baidu.com   //在同一个项目下生成另外一个spider，实际项目中会有个多个spider
在pycharm上面检查目录结构
在pycharm的terminal里面执行：
cd D:\xiaozhan_git\study_20190608\python3-crawler\python3_scrapy\quotetutorial\
scrapy crawl quotes      //开始爬取数据,就可以运行爬虫,quotes是spider的名称
多个spider如何运行？？

也可以这样运行spider,需要先进入到spider所在的目录：
scrapy runspider quotes.py
scrapy runspider baidu.py

//重点分析quotes.py，parse(self, response),这个方法里面的response就是爬取网站后得到的response，可以对它进行分析，获取信息
//爬虫启动的时候就会自动调用这个parse()方法，对网站内容进行解析

# study guide for scrapy
## https://docs.scrapy.org/en/latest/intro/tutorial.html
On Windows, use double quotes instead:
scrapy shell "http://quotes.toscrape.com/page/1/"

scrapy crawl quotes -o quotes.json   运行后会把结果存到一个json格式文件里面
scrapy crawl quotes -o quotes.jl   运行后会把结果存到一个json格式文件里面
scrapy crawl quotes -o quotes.csv   运行后会把结果存到一个csv格式文件里面
scrapy crawl quotes -o quotes.xml  运行后会把结果存到一个xml格式文件里面

也可以修改pipelines.py,实现一些功能，例如将数据存到MongoDB，或者json文件

# scrapy shell "https://docs.scrapy.org/en/latest/_static/selectors-sample1.html" 
response.css('a[href*=image]::attr(href)').getall()
>>> print(response.text)
<html>
 <head>
  <base href='http://example.com/' />
  <title>Example website</title>
 </head>
 <body>
  <div id='images'>
   <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' /></a>
   <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' /></a>
   <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' /></a>
   <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' /></a>
   <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' /></a>
  </div>
 </body>
</html>
# response.css('a::attr(href)').getall()
# response.css('a[href*=image] img::attr(src)').getall()
# response.css('a[href*=image]::attr(href)').getall()
# response.css('base::attr(href)').get()
#　[img.attrib['src'] for img in response.css('img')]

# 参考multiple这个工程，是设置了爬取两个网站的数据，仅仅是一个框架，细节还需要再增加
# 例如增加pipeline处理两个网站的数据等等
# 如何执行：进入到command目录下面，scrapy crawall 



